from nltk.tokenize import word_tokenize:
    # def tokensize():
    #txt = input("Enter something\n")
    # txt.split()

    # tokensize()

    # def tokensize(**user):
    #        print(user)

    #    tokensize(id=2, age=25, name="Hanzala")

text = "Hello everyone. Welcome to GeeksforGeeks."
word_tokenize(text)
